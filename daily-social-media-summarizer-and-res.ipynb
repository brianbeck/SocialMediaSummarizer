{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88817015",
   "metadata": {
    "papermill": {
     "duration": 0.008058,
     "end_time": "2025-04-18T23:28:42.813793",
     "exception": false,
     "start_time": "2025-04-18T23:28:42.805735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Daily Social Media Summarizer and Research Tool\n",
    "\n",
    "### Problem Statement:\n",
    "\n",
    "The average person uses [6.83 social media platform each month](https://datareportal.com/social-media-users) with the amount of people regularly getting their news from social media sites [continuing to grow](https://www.pewresearch.org/journalism/fact-sheet/social-media-and-news-fact-sheet/).\n",
    "\n",
    "How do we conslidate, summarize, and filter social media news in a single place to enable users to quickly understand what is happening and ask questions for further clarification, bringing in additional authoritative data sources? \n",
    "\n",
    "Additionally, is it possible to filter search results based on sentiment.  For example, can a user filter out negative news stories if they want to see positive news only and can we fine-tune the model to the user's preferences.\n",
    "\n",
    "### User Persona:\n",
    "\n",
    "Internet users of all backgrounds who get their news from social media services.\n",
    "\n",
    "### Proposed Solution\n",
    "\n",
    "We can build an an application which leverage LLM technology that:\n",
    "\n",
    "1. Pulls news from subscribed social media sites on a daily basis\n",
    "3. Classifies the returned news stories based on sentiment\n",
    "4. Creates embeddings for the news stories and stores it in a vector database with the classification metadata\n",
    "5. Create a summary for the user of news stories in the past 24 hours meeting, filtering for negative sentiment\n",
    "6. Allows the user to query for more information on the news stories, while providing grounding when necessary\n",
    "\n",
    "### In this notebook\n",
    "\n",
    "I demonstrate this possible solution in this notebook by :\n",
    "\n",
    "1. Using the Reddit API to pull submissions from the past day\n",
    "2. Categorizing the returned submissions based on sentiment\n",
    "3. Creating embeddings for the submissions and store them in a vector database with their metadata\n",
    "4. Search a vector database for all positive submissions in the past day which don't have negative sentiment and return an summary of the posts\n",
    "5. Allow the user to further ask questions.  These questions can span the social media posts in the vector database or use grounding\n",
    "\n",
    "### Concepts Utilized From the Course Include:\n",
    "\n",
    "1. Prompting - Role prompting, System prompting, one-shot, and few-shot prompting\n",
    "2. Structured Output\n",
    "3. Fine Tuning\n",
    "4. Embeddings\n",
    "5. RAG\n",
    "6. Grounding\n",
    "\n",
    "### Differences / Expansion from the course material:\n",
    "\n",
    "1. I used an LLM to create synthetic training and test data to fine tune a model.\n",
    "2. I classify submissions and add this metadata when inserting the document embeddings into the vector database\n",
    "3. I added extensive metadata for the document embeddings stored in the vector database\n",
    "4. I perform queries on the vector database using constraints based on the metadata\n",
    "\n",
    "### FAQ:\n",
    "\n",
    "#### Why is RAG important in this use case?\n",
    "I want to provide the user the capability to summarize and search social media submission across all of their social media networks. This enables them to subscribe to content specific to their interests and see this content in one place. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd0ffb2",
   "metadata": {
    "papermill": {
     "duration": 0.007154,
     "end_time": "2025-04-18T23:28:42.828648",
     "exception": false,
     "start_time": "2025-04-18T23:28:42.821494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set Up\n",
    "\n",
    "1. Remove Conflicting Dependencies\n",
    "2. Install Google Gen AI, Chroma DB, and dependencies\n",
    "3. Install Rediit SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49159ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:28:42.844740Z",
     "iopub.status.busy": "2025-04-18T23:28:42.844342Z",
     "iopub.status.idle": "2025-04-18T23:29:35.526333Z",
     "shell.execute_reply": "2025-04-18T23:29:35.524707Z"
    },
    "papermill": {
     "duration": 52.692612,
     "end_time": "2025-04-18T23:29:35.528593",
     "exception": false,
     "start_time": "2025-04-18T23:28:42.835981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.2/135.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.9/454.9 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -qqy jupyterlab google-cloud-automl # Remove conflicting packages\n",
    "\n",
    "!pip install -U -q \"google-genai==1.7.0\" \"chromadb==0.6.3\" \"protobuf==3.20.3\" \"google-api-core==2.16.2\" \"praw==7.8.1\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757b713",
   "metadata": {
    "papermill": {
     "duration": 0.0091,
     "end_time": "2025-04-18T23:29:35.548447",
     "exception": false,
     "start_time": "2025-04-18T23:29:35.539347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Check Set-Up\n",
    "\n",
    "Check to make sure we are using the correct version of the Google SDK to ensure compatability and the environment is set-up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5496942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:35.568407Z",
     "iopub.status.busy": "2025-04-18T23:29:35.567969Z",
     "iopub.status.idle": "2025-04-18T23:29:36.828145Z",
     "shell.execute_reply": "2025-04-18T23:29:36.827193Z"
    },
    "papermill": {
     "duration": 1.272213,
     "end_time": "2025-04-18T23:29:36.829847",
     "exception": false,
     "start_time": "2025-04-18T23:29:35.557634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "genai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c90374d",
   "metadata": {
    "papermill": {
     "duration": 0.009553,
     "end_time": "2025-04-18T23:29:36.849299",
     "exception": false,
     "start_time": "2025-04-18T23:29:36.839746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Set up the Google API key\n",
    "\n",
    "To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n",
    "\n",
    "If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n",
    "\n",
    "To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fc8dc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:36.870061Z",
     "iopub.status.busy": "2025-04-18T23:29:36.869524Z",
     "iopub.status.idle": "2025-04-18T23:29:36.985379Z",
     "shell.execute_reply": "2025-04-18T23:29:36.984272Z"
    },
    "papermill": {
     "duration": 0.128446,
     "end_time": "2025-04-18T23:29:36.987365",
     "exception": false,
     "start_time": "2025-04-18T23:29:36.858919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3512153b",
   "metadata": {
    "papermill": {
     "duration": 0.009429,
     "end_time": "2025-04-18T23:29:37.006418",
     "exception": false,
     "start_time": "2025-04-18T23:29:36.996989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fine Tuning\n",
    "\n",
    "I started by fine-tuning a model to classify the sentiment of a social media post.  To do this, I performed the followign tasks.\n",
    "\n",
    "1. Finding a gemini model that allows training\n",
    "2. Creating synthetic dataset of generated posts and sentitments to train and test the model\n",
    "3. Prepare and sample the data\n",
    "4. Evaluate baseline performance of the default model\n",
    "5. Fine-Tune the Model\n",
    "6. Evaluate performance of the fine-tuned model\n",
    "\n",
    "However, the baseline performance of the default LLM was near or at 100% in multiple test runs and 35% to 75% for the fine tuned model. The default nodel was better! I have documented this experiment in this [Kaggle notebook](https://www.kaggle.com/code/brbeck/fine-tuning-that-made-things-worse)\n",
    "\n",
    "For this exercise we will use the default LLM, but in a customer available production version of this application, we can use real world submissions and user classification to fine-tune a model to better align with user sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f24b40",
   "metadata": {
    "papermill": {
     "duration": 0.009174,
     "end_time": "2025-04-18T23:29:37.024994",
     "exception": false,
     "start_time": "2025-04-18T23:29:37.015820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Connect to the Google Gemini API\n",
    "\n",
    "We will use the default model which supports fine-tuning to query gemini for sentiment for a submission.  As noted above, the default model performed well with success rates as high as 100% in testing.  In a customer available version of a production application, we could use real world submissions and the user's sentiment scoring to fine-tune a model and provide categorization more aligned with a user's sentiment.\n",
    "\n",
    "In the query below, we structure the output to conform to an enum to return 'positive', 'negative', or 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f24d2a93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:37.046164Z",
     "iopub.status.busy": "2025-04-18T23:29:37.045787Z",
     "iopub.status.idle": "2025-04-18T23:29:37.708506Z",
     "shell.execute_reply": "2025-04-18T23:29:37.707429Z"
    },
    "papermill": {
     "duration": 0.675186,
     "end_time": "2025-04-18T23:29:37.710411",
     "exception": false,
     "start_time": "2025-04-18T23:29:37.035225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "import enum\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "model_config = types.GenerateContentConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=1,\n",
    "    max_output_tokens=5,\n",
    ")\n",
    "\n",
    "zero_shot_prompt = \"\"\"You are an expert at classifying the sentiment of text written by people and posted on the internet as submissions.  There are 3 classifications: positive, neutral, or negative.  \n",
    "positive submissions are hopeful, confident, and cover good aspects of a situation.  negative submissions concern war, violence, and crime.  neutral submissions are neither positive or negative. \n",
    "Please classify the sentiment of each of the following Reddit submissions as positive, neutral or negative.\"\"\"\n",
    "\n",
    "def eval_submission(submission):\n",
    "    \n",
    "    prompt = zero_shot_prompt\n",
    "    prompt += \"Submission:\" + submission\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        #model='gemini-2.0-flash',\n",
    "        model='gemini-1.5-flash-001',\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type=\"text/x.enum\",\n",
    "            response_schema=Sentiment\n",
    "        ),\n",
    "        contents=prompt)\n",
    "    \n",
    "    return response.text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e2488",
   "metadata": {
    "papermill": {
     "duration": 0.009121,
     "end_time": "2025-04-18T23:29:37.729082",
     "exception": false,
     "start_time": "2025-04-18T23:29:37.719961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Social Media Post RAG\n",
    "\n",
    "Now that we have created our fine-tuned model, we can use it on social media posts to classify the sentiment and filter posts for the user. \n",
    "\n",
    "In addition, we can create a RAG architecture where we create embeddingd for the posts and store them, along with post metadata into a vector database.\n",
    "\n",
    "This will allow us summarize the social media posts as intended, but to enable users to search over their post history, filter posts based on sentiment (they may not want to see sad news), date posted or other metadata.\n",
    "\n",
    "To do this, we will:\n",
    "\n",
    "1. Create a Reddit API Caller\n",
    "2. Retrieve social media posts and post metadata from Reddit\n",
    "3. Use our fine tuned model to classify the sentiment and update the metadata\n",
    "4. Create the embeddings for the posts and add them and their metadata to the vector database\n",
    "5. Search the database for all postings in the previous 24 hours with a positive sentiment\n",
    "6. Use Gemini to create a summary of the posts meeting that criteria\n",
    "\n",
    "Additionally, in the future, we can allow users to classify the sentiment of their social media posts and use this information to fine-tune a model for them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d2e02a",
   "metadata": {
    "papermill": {
     "duration": 0.009089,
     "end_time": "2025-04-18T23:29:37.747577",
     "exception": false,
     "start_time": "2025-04-18T23:29:37.738488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Set Up Reddit API Caller\n",
    "\n",
    "To create a Reddit API caller account, you must create a new application on reddit [here](https://www.reddit.com/prefs/apps/).\n",
    "\n",
    "To make the keys available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee87b768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:37.767473Z",
     "iopub.status.busy": "2025-04-18T23:29:37.766876Z",
     "iopub.status.idle": "2025-04-18T23:29:38.112231Z",
     "shell.execute_reply": "2025-04-18T23:29:38.111165Z"
    },
    "papermill": {
     "duration": 0.357384,
     "end_time": "2025-04-18T23:29:38.114116",
     "exception": false,
     "start_time": "2025-04-18T23:29:37.756732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "REDDIT_CLIENT_ID = UserSecretsClient().get_secret(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = UserSecretsClient().get_secret(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USERNAME = UserSecretsClient().get_secret(\"REDDIT_USERNAME\")\n",
    "REDDIT_PASSWORD = UserSecretsClient().get_secret(\"REDDIT_PASSWORD\")\n",
    "\n",
    "if not all([REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USERNAME, REDDIT_PASSWORD]):\n",
    "    print(\"Please set REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USERNAME, and REDDIT_PASSWORD environment variables\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfedec6",
   "metadata": {
    "papermill": {
     "duration": 0.00902,
     "end_time": "2025-04-18T23:29:38.132644",
     "exception": false,
     "start_time": "2025-04-18T23:29:38.123624",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Pull the Submissions\n",
    "\n",
    "For the subreddits a user has subscribed to, we can pull the submissions for each subreddit.\n",
    "\n",
    "In our example below, we reduce the amount of stories being pulled to accomodate the free tier of API calling.\n",
    "\n",
    "For each submission we are not only retrieving the body of the submission, but also the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29efb1e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:38.152559Z",
     "iopub.status.busy": "2025-04-18T23:29:38.152187Z",
     "iopub.status.idle": "2025-04-18T23:29:41.003170Z",
     "shell.execute_reply": "2025-04-18T23:29:41.002024Z"
    },
    "papermill": {
     "duration": 2.863092,
     "end_time": "2025-04-18T23:29:41.004918",
     "exception": false,
     "start_time": "2025-04-18T23:29:38.141826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents 11\n"
     ]
    }
   ],
   "source": [
    "# Import Reddit interface\n",
    "import praw\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "\n",
    "# The number of subreddits we will return from the API\n",
    "SUBREDDIT_LIMIT = 10  # None would remove te limit\n",
    "# The number of submissions we will retrieve from each subreddit\n",
    "SUBMISSION_LIMIT = 2\n",
    "\n",
    "\n",
    "# Retrieve the list of subbreddits for a given user\n",
    "def get_user_subreddits(reddit: praw.Reddit, username: str) -> List[str]:\n",
    "\n",
    "    user = reddit.redditor(username)\n",
    "    subreddits = reddit.user.subreddits(limit=SUBREDDIT_LIMIT);\n",
    "\n",
    "    return list(subreddits)\n",
    "\n",
    "# Initialize Reddit interface and retrieve auth token\n",
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    username=REDDIT_USERNAME,\n",
    "    password=REDDIT_PASSWORD,\n",
    "    user_agent='python:reddit-scraper:v1.0 (by + /u/' + REDDIT_USERNAME + ')',\n",
    ")\n",
    "\n",
    "subreddits = get_user_subreddits(reddit, REDDIT_USERNAME)\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    #print(f\"\\nFetching recent posts from r/{subreddit}...\")\n",
    "    \n",
    "    for submission in subreddit.new(limit=SUBMISSION_LIMIT):\n",
    "        document = {\n",
    "            'body': submission.selftext\n",
    "        }\n",
    "\n",
    "        metadata = {\n",
    "            'source': \"reddit\",\n",
    "            'title': submission.title,\n",
    "            'author': str(submission.author),\n",
    "            'score': submission.score,\n",
    "            'created_utc': submission.created_utc,\n",
    "            'url': submission.url,\n",
    "            'num_comments': submission.num_comments,\n",
    "            'body': submission.selftext,\n",
    "            'category': subreddit.name,\n",
    "            'sentiment': \"none\"\n",
    "        }\n",
    "\n",
    "        #Remove link posts for this exercise. In a more robust implementation we would follow the links, scrape the site and extract the text for embeddings and classification\n",
    "        if document['body'] != '':\n",
    "            documents.append(document['body'])\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "print(\"Number of Documents \" + str(len(documents)))\n",
    "\n",
    "\n",
    "# Need to add sample data here and check of the reddit connection is valid or not.  If not, use the sample data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f1ed05",
   "metadata": {
    "papermill": {
     "duration": 0.010613,
     "end_time": "2025-04-18T23:29:41.025248",
     "exception": false,
     "start_time": "2025-04-18T23:29:41.014635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sentiment Classification\n",
    "\n",
    "Here I will classify the sentiment using the default LLM we can use for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b6e4437",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:41.046161Z",
     "iopub.status.busy": "2025-04-18T23:29:41.045785Z",
     "iopub.status.idle": "2025-04-18T23:29:44.943591Z",
     "shell.execute_reply": "2025-04-18T23:29:44.942592Z"
    },
    "papermill": {
     "duration": 3.910902,
     "end_time": "2025-04-18T23:29:44.945539",
     "exception": false,
     "start_time": "2025-04-18T23:29:41.034637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "# Define a helper to retry when per-minute quota is reached.\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "@retry.Retry(predicate=is_retriable)\n",
    "def classify_text(text: str) -> str:\n",
    "    \"\"\"Classify the sentiment of the provided text\"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=model_id, contents=text)\n",
    "    rc = response.candidates[0]\n",
    "\n",
    "    # Any errors, filters, recitation, etc we can mark as a general error\n",
    "    if rc.finish_reason.name != \"STOP\":\n",
    "        return \"(error)\"\n",
    "    else:\n",
    "        return rc.content.parts[0].text\n",
    "\n",
    "\n",
    "for document, metadata in zip(documents, metadatas):\n",
    "    metadata['sentiment'] = eval_submission(document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92dc77a",
   "metadata": {
    "papermill": {
     "duration": 0.009205,
     "end_time": "2025-04-18T23:29:44.964355",
     "exception": false,
     "start_time": "2025-04-18T23:29:44.955150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's view the sentiment for the social media submissions. \n",
    "\n",
    "If none of the submissions have a positive sentiment, we make them all positive for the purposes of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0629b542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:44.984563Z",
     "iopub.status.busy": "2025-04-18T23:29:44.984201Z",
     "iopub.status.idle": "2025-04-18T23:29:44.991934Z",
     "shell.execute_reply": "2025-04-18T23:29:44.990675Z"
    },
    "papermill": {
     "duration": 0.019714,
     "end_time": "2025-04-18T23:29:44.993589",
     "exception": false,
     "start_time": "2025-04-18T23:29:44.973875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n",
      "neutral\n",
      "negative\n",
      "neutral\n",
      "neutral\n",
      "negative\n",
      "neutral\n",
      "positive\n",
      "neutral\n",
      "neutral\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "positive_submissions = 0\n",
    "\n",
    "for document, metadata in zip(documents, metadatas):\n",
    "    if (metadata['sentiment'] == 'positive'):\n",
    "        positive_submissions += 1\n",
    "    print(metadata['sentiment'])\n",
    "\n",
    "if (positive_submissions == 0):\n",
    "    print (\"NO POSITIVE SUBMISSIONS!! MAKING THEM ALL POSITIVE FOR THIS EXERCISE\")\n",
    "    for document, metadata in zip(documents, metadatas):\n",
    "        metadata['sentiment'] = 'positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d950086",
   "metadata": {
    "papermill": {
     "duration": 0.009318,
     "end_time": "2025-04-18T23:29:45.012752",
     "exception": false,
     "start_time": "2025-04-18T23:29:45.003434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Explore available models to calculate embeddings\n",
    "\n",
    "You will be using the [`embedContent`](https://ai.google.dev/api/embeddings#method:-models.embedcontent) API method to calculate embeddings in this guide. Find a model that supports it through the [`models.list`](https://ai.google.dev/api/models#method:-models.list) endpoint. You can also find more information about the embedding models on [the models page](https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding).\n",
    "\n",
    "`text-embedding-004` is the most recent generally-available embedding model, so you will use it for this exercise, but try out the experimental `gemini-embedding-exp-03-07` model too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acc91cca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:45.033813Z",
     "iopub.status.busy": "2025-04-18T23:29:45.033472Z",
     "iopub.status.idle": "2025-04-18T23:29:45.233442Z",
     "shell.execute_reply": "2025-04-18T23:29:45.232333Z"
    },
    "papermill": {
     "duration": 0.21248,
     "end_time": "2025-04-18T23:29:45.235211",
     "exception": false,
     "start_time": "2025-04-18T23:29:45.022731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "for m in client.models.list():\n",
    "    if \"embedContent\" in m.supported_actions:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01471d94",
   "metadata": {
    "papermill": {
     "duration": 0.009278,
     "end_time": "2025-04-18T23:29:45.254677",
     "exception": false,
     "start_time": "2025-04-18T23:29:45.245399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating the embedding database with ChromaDB\n",
    "\n",
    "I create a [custom function](https://docs.trychroma.com/guides/embeddings#custom-embedding-functions) to generate embeddings with the Gemini API. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4226e444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:45.276718Z",
     "iopub.status.busy": "2025-04-18T23:29:45.276322Z",
     "iopub.status.idle": "2025-04-18T23:29:45.993287Z",
     "shell.execute_reply": "2025-04-18T23:29:45.992105Z"
    },
    "papermill": {
     "duration": 0.730665,
     "end_time": "2025-04-18T23:29:45.995457",
     "exception": false,
     "start_time": "2025-04-18T23:29:45.264792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from google.api_core import retry\n",
    "\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "# Define a helper to retry when per-minute quota is reached.\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    # Specify whether to generate embeddings for documents, or queries\n",
    "    document_mode = True\n",
    "\n",
    "    @retry.Retry(predicate=is_retriable)\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        if self.document_mode:\n",
    "            embedding_task = \"retrieval_document\"\n",
    "        else:\n",
    "            embedding_task = \"retrieval_query\"\n",
    "\n",
    "        response = client.models.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            contents=input,\n",
    "            config=types.EmbedContentConfig(\n",
    "                task_type=embedding_task,\n",
    "            ),\n",
    "        )\n",
    "        return [e.values for e in response.embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41c1fcc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:46.019035Z",
     "iopub.status.busy": "2025-04-18T23:29:46.018625Z",
     "iopub.status.idle": "2025-04-18T23:29:46.726538Z",
     "shell.execute_reply": "2025-04-18T23:29:46.725235Z"
    },
    "papermill": {
     "duration": 0.722506,
     "end_time": "2025-04-18T23:29:46.728532",
     "exception": false,
     "start_time": "2025-04-18T23:29:46.006026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "DB_NAME = \"redditposts\"\n",
    "\n",
    "embed_fn = GeminiEmbeddingFunction()\n",
    "embed_fn.document_mode = True\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
    "\n",
    "db.add(documents=documents, metadatas=metadatas, ids=[str(i) for i in range(len(documents))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82322d3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:46.751001Z",
     "iopub.status.busy": "2025-04-18T23:29:46.750614Z",
     "iopub.status.idle": "2025-04-18T23:29:46.759571Z",
     "shell.execute_reply": "2025-04-18T23:29:46.758518Z"
    },
    "papermill": {
     "duration": 0.022175,
     "end_time": "2025-04-18T23:29:46.761636",
     "exception": false,
     "start_time": "2025-04-18T23:29:46.739461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.count()\n",
    "# You can peek at the data too.\n",
    "# db.peek(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1b874e",
   "metadata": {
    "papermill": {
     "duration": 0.009664,
     "end_time": "2025-04-18T23:29:46.781194",
     "exception": false,
     "start_time": "2025-04-18T23:29:46.771530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create a Summary\n",
    "\n",
    "Here I create a summary of all submissions from the past 24 hours that have a positive sentiment.  I do this by using Chroma Metadata filtering which enables us to filter on date and sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b8508",
   "metadata": {
    "papermill": {
     "duration": 0.009889,
     "end_time": "2025-04-18T23:29:46.801383",
     "exception": false,
     "start_time": "2025-04-18T23:29:46.791494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First, we get the list of submission with positive intent in the past 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a6bdf98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:46.822955Z",
     "iopub.status.busy": "2025-04-18T23:29:46.822560Z",
     "iopub.status.idle": "2025-04-18T23:29:46.835445Z",
     "shell.execute_reply": "2025-04-18T23:29:46.834156Z"
    },
    "papermill": {
     "duration": 0.025846,
     "end_time": "2025-04-18T23:29:46.837465",
     "exception": false,
     "start_time": "2025-04-18T23:29:46.811619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents within the previous time period and with positive sentiment 1\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "LOOKBACK_IN_DAYS = 1;\n",
    "\n",
    "starttime = datetime.now()\n",
    "endtime = starttime - timedelta(days=LOOKBACK_IN_DAYS)\n",
    "\n",
    "results_within_timeframe = db.get(where={\"$and\": [{\"created_utc\": {\"$gt\": int(endtime.timestamp())}},{\"sentiment\": {\"$eq\": \"positive\"}}]})\n",
    "\n",
    "all_passages = results_within_timeframe[\"documents\"]\n",
    "\n",
    "print(\"Number of Documents within the previous time period and with positive sentiment \" + str(len(results_within_timeframe['documents'])))\n",
    "\n",
    "#for i in results_within_timeframe['metadatas']:\n",
    "#    print(i['created_utc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a29c8e",
   "metadata": {
    "papermill": {
     "duration": 0.009587,
     "end_time": "2025-04-18T23:29:46.857108",
     "exception": false,
     "start_time": "2025-04-18T23:29:46.847521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now I create the prompt to generate the final summary from the retreived data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88cd7715",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:46.878487Z",
     "iopub.status.busy": "2025-04-18T23:29:46.878085Z",
     "iopub.status.idle": "2025-04-18T23:29:46.883610Z",
     "shell.execute_reply": "2025-04-18T23:29:46.882339Z"
    },
    "papermill": {
     "duration": 0.018197,
     "end_time": "2025-04-18T23:29:46.885535",
     "exception": false,
     "start_time": "2025-04-18T23:29:46.867338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful and informative bot that provides a succint summary of each document provided provided below.\n",
      "Be sure to respond in a complete sentence, with one bullet point for each document.\n",
      "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \n",
      "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
      "PASSAGE: I messaged the mods but haven’t heard back, so please let me know if this is out of line. Many CoastFIRE calculators are either too simple or overly complex. I created one to fill the gap—a CoastFIRE calculator I wish existed. It not only determines if you’ve reached CoastFIRE but also allows scenario analysis for reduced variable expenses, part-time income (Barista FI), and paying off your mortgage early with a lump sum at retirement age. It includes notifications about being ahead of schedule and how much earlier you could retire. I've also included net worth if you were to continue investing. I'm going to continue tweaking this over time and add features.   There are no ads, and I don’t plan to add any for now. For transparency, I’ve included a “Buy me a coffee” link. I’m open to feedback and suggestions for new features.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\n",
    "prompt = f\"\"\"You are a helpful and informative bot that provides a succint summary of each document provided provided below.\n",
    "Be sure to respond in a complete sentence, with one bullet point for each document.\n",
    "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \n",
    "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
    "\"\"\"\n",
    "\n",
    "# Add the retrieved documents to the prompt.\n",
    "for passage in all_passages:\n",
    "    passage_oneline = passage.replace(\"\\n\", \" \")\n",
    "    prompt += f\"PASSAGE: {passage_oneline}\\n\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc08a0b",
   "metadata": {
    "papermill": {
     "duration": 0.009745,
     "end_time": "2025-04-18T23:29:46.905611",
     "exception": false,
     "start_time": "2025-04-18T23:29:46.895866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, we pass the prompt, with retrieved data, to the model to create the social media summary.\n",
    "\n",
    "In an application setting, we could provide the user will all data filtered by sentiment with the ability to change the sentiment for future RAG retrieval filtering and future fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49f3fd82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:46.927538Z",
     "iopub.status.busy": "2025-04-18T23:29:46.927175Z",
     "iopub.status.idle": "2025-04-18T23:29:47.788479Z",
     "shell.execute_reply": "2025-04-18T23:29:47.787414Z"
    },
    "papermill": {
     "duration": 0.87426,
     "end_time": "2025-04-18T23:29:47.790174",
     "exception": false,
     "start_time": "2025-04-18T23:29:46.915914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, I'd be happy to summarize that for you! It sounds like the author created a CoastFIRE calculator to help people figure out if they're on track to retire comfortably. Here's the gist:\n",
       "\n",
       "*   The author made a CoastFIRE calculator that's more detailed than basic ones but not overly complicated, helping users see if they're on track for CoastFIRE (reaching a point where their investments will grow enough to cover retirement without further contributions).\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=prompt)\n",
    "\n",
    "Markdown(answer.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6092f60",
   "metadata": {
    "papermill": {
     "duration": 0.009718,
     "end_time": "2025-04-18T23:29:47.810105",
     "exception": false,
     "start_time": "2025-04-18T23:29:47.800387",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Search Across with RAG\n",
    "\n",
    "Once a user sees the summary, they can perform a RAG search which uses embeddings to retrieve relevant information and use it in the prompt for consideration.\n",
    "\n",
    "The bleow code block searches the vector database for embeddings relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e931b7f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:47.831255Z",
     "iopub.status.busy": "2025-04-18T23:29:47.830803Z",
     "iopub.status.idle": "2025-04-18T23:29:48.019739Z",
     "shell.execute_reply": "2025-04-18T23:29:48.018578Z"
    },
    "papermill": {
     "duration": 0.201776,
     "end_time": "2025-04-18T23:29:48.021710",
     "exception": false,
     "start_time": "2025-04-18T23:29:47.819934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Been looking at them for a while and trying to see if they are worth the investment. Seems they are built and growing despite all the negative publicity about their IPO. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch to query mode when generating embeddings.\n",
    "embed_fn.document_mode = False\n",
    "\n",
    "# Search the Chroma DB using the specified query.\n",
    "query = \"What is the latest news today on Google Artificial Intelligence?\"\n",
    "\n",
    "result = db.query(query_texts=[query], n_results=1)\n",
    "[all_passages] = result[\"documents\"]\n",
    "\n",
    "Markdown(all_passages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0097b1",
   "metadata": {
    "papermill": {
     "duration": 0.009759,
     "end_time": "2025-04-18T23:29:48.041770",
     "exception": false,
     "start_time": "2025-04-18T23:29:48.032011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A prompt is created which incorprates the document sections returned from the vector database search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e16b078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:48.063176Z",
     "iopub.status.busy": "2025-04-18T23:29:48.062770Z",
     "iopub.status.idle": "2025-04-18T23:29:48.068445Z",
     "shell.execute_reply": "2025-04-18T23:29:48.067366Z"
    },
    "papermill": {
     "duration": 0.018177,
     "end_time": "2025-04-18T23:29:48.069958",
     "exception": false,
     "start_time": "2025-04-18T23:29:48.051781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful and informative bot that answers questions using text from the reference passage included below. \n",
      "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \n",
      "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \n",
      "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
      "\n",
      "QUESTION: What is the latest news today on Google Artificial Intelligence?\n",
      "PASSAGE: Been looking at them for a while and trying to see if they are worth the investment. Seems they are built and growing despite all the negative publicity about their IPO. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_oneline = query.replace(\"\\n\", \" \")\n",
    "\n",
    "# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\n",
    "prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below. \n",
    "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \n",
    "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \n",
    "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
    "\n",
    "QUESTION: {query_oneline}\n",
    "\"\"\"\n",
    "\n",
    "# Add the retrieved documents to the prompt.\n",
    "for passage in all_passages:\n",
    "    passage_oneline = passage.replace(\"\\n\", \" \")\n",
    "    prompt += f\"PASSAGE: {passage_oneline}\\n\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053b375",
   "metadata": {
    "papermill": {
     "duration": 0.009995,
     "end_time": "2025-04-18T23:29:48.090634",
     "exception": false,
     "start_time": "2025-04-18T23:29:48.080639",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I then pass that prompt to the LLM to generate a response for the user. If the passages don't contain information helpful to the user, the LLM will respond as such.  However, the user can also search using grounding and incorporate information from a Google search as outlined in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9116a9a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:48.112542Z",
     "iopub.status.busy": "2025-04-18T23:29:48.112111Z",
     "iopub.status.idle": "2025-04-18T23:29:48.527367Z",
     "shell.execute_reply": "2025-04-18T23:29:48.526320Z"
    },
    "papermill": {
     "duration": 0.428195,
     "end_time": "2025-04-18T23:29:48.529247",
     "exception": false,
     "start_time": "2025-04-18T23:29:48.101052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I am sorry, but the provided passage does not contain any information about Google Artificial Intelligence. Therefore, I cannot answer your question.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=prompt)\n",
    "\n",
    "Markdown(answer.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f642e482",
   "metadata": {
    "papermill": {
     "duration": 0.009896,
     "end_time": "2025-04-18T23:29:48.549445",
     "exception": false,
     "start_time": "2025-04-18T23:29:48.539549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Search with Grounding\n",
    "\n",
    "The user may want to discover more about a topic from a social media post, but this information may be very recent and unknown to the LLM. We can use grounding via Google search to get the latest information for the topic the user is researching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c64f37bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T23:29:48.570995Z",
     "iopub.status.busy": "2025-04-18T23:29:48.570657Z",
     "iopub.status.idle": "2025-04-18T23:29:51.428888Z",
     "shell.execute_reply": "2025-04-18T23:29:51.427753Z"
    },
    "papermill": {
     "duration": 2.87115,
     "end_time": "2025-04-18T23:29:51.430748",
     "exception": false,
     "start_time": "2025-04-18T23:29:48.559598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's the latest on Google AI, as of today, April 18, 2025:\n",
       "\n",
       "**Key Developments:**\n",
       "\n",
       "*   **Gemini 2.5 Flash:** Google has introduced Gemini 2.5 Flash, their first fully hybrid reasoning model that allows developers to switch thinking on or off. It's optimized for low latency and cost efficiency, making it suitable for real-time customer interactions.\n",
       "*   **DolphinGemma:** Google AI is using the DolphinGemma language model to help decode dolphin communication.\n",
       "*   **InstructPipe:** New research focuses on generating visual blocks pipelines using human instructions and LLMs.\n",
       "*   **Gemma 3 QAT Models:** The latest open AI models are being brought to consumer GPUs.\n",
       "\n",
       "**AI in Google Workspace:**\n",
       "\n",
       "*   Google is bringing more AI capabilities to Workspace tools like Docs, Sheets, Meet, and Chat.\n",
       "*   They've introduced Google Workspace Flows, a new way to automate and orchestrate work across apps using AI.\n",
       "*   New audio features are coming to Docs, including full audio versions of documents and podcast-style overviews.\n",
       "\n",
       "**Google Cloud:**\n",
       "\n",
       "*   Google is offering new video, image, speech, and music generative AI tools for Vertex AI.\n",
       "\n",
       "**AI for Developers:**\n",
       "\n",
       "*   The Google Developer Program is offering enhanced coding assistance with Gemini Code Assist Standard.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_with_search = types.GenerateContentConfig(\n",
    "    tools=[types.Tool(google_search=types.GoogleSearch())],\n",
    ")\n",
    "\n",
    "def query_with_grounding():\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=query,\n",
    "        config=config_with_search,\n",
    "    )\n",
    "    return response.candidates[0]\n",
    "\n",
    "\n",
    "rc = query_with_grounding()\n",
    "Markdown(rc.content.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8aeaf0",
   "metadata": {
    "papermill": {
     "duration": 0.012947,
     "end_time": "2025-04-18T23:29:51.459615",
     "exception": false,
     "start_time": "2025-04-18T23:29:51.446668",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The capabilities demonstrated above show that it is possible to build an application that:\n",
    "\n",
    "1. Pulls news from subscribed social media sites on a daily basis\n",
    "3. Classifies the returned news stories based on user sentiment\n",
    "4. Creates embeddings for the news stories and stores it in a vector database with the classification metadata\n",
    "5. Create a summary for the user of news stories in the past 24 hours meeting, filtering for negative sentiment\n",
    "6. Allows the user to query for more information on the news stories, while providing grounding when necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f2b1c",
   "metadata": {
    "papermill": {
     "duration": 0.010612,
     "end_time": "2025-04-18T23:29:51.484199",
     "exception": false,
     "start_time": "2025-04-18T23:29:51.473587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Next Steps\n",
    "\n",
    "My next goal is to create an agent which performs the actions above as an exercise in agent creation.  From there I would be interested to create a desktop or web application with a deployed MLOPS backend to support the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581aa5fb",
   "metadata": {
    "papermill": {
     "duration": 0.010115,
     "end_time": "2025-04-18T23:29:51.505446",
     "exception": false,
     "start_time": "2025-04-18T23:29:51.495331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "These are sources where I have sourced code, examples, or information to help me with this notebook.\n",
    "\n",
    "### Google Course Notebooks\n",
    "\n",
    "1. Day 1 - Prompting\n",
    "2. Day 2 - Document Q&A with RAG\n",
    "3. Day 3 - Building an agent with LangGraph\n",
    "4. Day 4 - Fine tuning a custom model\n",
    "\n",
    "### Additional References\n",
    "\n",
    "1. Chroma Metadata Filtering : [link](https://docs.trychroma.com/docs/querying-collections/metadata-filtering)\n",
    "2. Sentiment Analysis with Keras and LSTM : [link](https://www.kaggle.com/code/roblexnana/sentiment-analysis-with-keras-and-lstm/notebook)\n",
    "3. Fine-tuning with the Gemini API : [link](https://ai.google.dev/gemini-api/docs/model-tuning)\n",
    "5. Fine-tuning Tutorial : [link](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python)\n",
    "6. Custom Embedding Functions : [link](https://docs.trychroma.com/docs/embeddings/embedding-functions#custom-embedding-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e541e",
   "metadata": {
    "papermill": {
     "duration": 0.036588,
     "end_time": "2025-04-18T23:29:51.553273",
     "exception": false,
     "start_time": "2025-04-18T23:29:51.516685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Additional Reading\n",
    "\n",
    "These are sources I referenced, but did not incorporate into this notebook. I note them here as additional reference materials for people who may be interested in social media submission classification in adjacent domains.\n",
    "\n",
    "1. Financial Sentiment Analysis: Techniques and Application : [link](https://dl.acm.org/doi/10.1145/3649451)\n",
    "2. Applying Sentiment Analysis Techniques in Social Media Data Threat of Armed Conflicts Using Two Times Series Models : [link](https://www.researchgate.net/publication/367142985_Applying_Sentiment_Analysis_Techniques_in_Social_Media_Data_About_Threat_of_Armed_Conflicts_Using_Two_Times_Series_Models)\n",
    "3. Threat detection in online discussion using convolutional neural network : [link](https://www.duo.uio.no/bitstream/handle/10852/59278/5/Thesis_Stenberg.pdf)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 73.33606,
   "end_time": "2025-04-18T23:29:52.685960",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-18T23:28:39.349900",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
